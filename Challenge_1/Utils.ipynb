{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Enviroment Setting"],"metadata":{"id":"MZyTSdsi-uT-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8VbNnFr52MQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624092805,"user_tz":-60,"elapsed":19964,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"0429ab97-28d9-483d-cccc-e45a063e0d7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","source":["%cd /gdrive/MyDrive/AN2DL-challenge-2022-polimi/"],"metadata":{"id":"0hqMK_2t6OiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624092806,"user_tz":-60,"elapsed":45,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"60c8dc3c-7bfa-4df0-91fd-ab3229285585"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/.shortcut-targets-by-id/1Pu-dfIBpuqbYCNKRZsNFDRCnCWgmOpf1/AN2DL-challenge-2022-polimi\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","from datetime import datetime\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from PIL import Image\n","\n","import tensorflow as tf\n","from keras.layers import Dropout\n","from keras.layers import BatchNormalization\n","from keras.regularizers import l2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)"],"metadata":{"id":"8uDdp3Ki6RD-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624096519,"user_tz":-60,"elapsed":3721,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"eae8bdbd-6764-4c00-df2f-40135198b6fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}]},{"cell_type":"code","source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"yQstHse26S88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Splitting"],"metadata":{"id":"24sBOT5r_NnV"}},{"cell_type":"code","source":["!pip install split-folders\n","import splitfolders"],"metadata":{"id":"NqzaLWbN6TB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624101153,"user_tz":-60,"elapsed":4641,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"257a870e-21dc-4d13-988b-986dbfb28faa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n"]}]},{"cell_type":"code","source":["!find ./Dataset/training_data_final/ -type d -print -exec sh -c \"ls {} | wc -l | head -n1\" \\;"],"metadata":{"id":"WgZa1rf26TG8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624109943,"user_tz":-60,"elapsed":8797,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"bb11c803-5506-45f4-c17b-1370ca7e0d69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./Dataset/training_data_final/\n","8\n","./Dataset/training_data_final/Species7\n","537\n","./Dataset/training_data_final/Species8\n","508\n","./Dataset/training_data_final/Species1\n","186\n","./Dataset/training_data_final/Species6\n","222\n","./Dataset/training_data_final/Species3\n","515\n","./Dataset/training_data_final/Species4\n","511\n","./Dataset/training_data_final/Species5\n","531\n","./Dataset/training_data_final/Species2\n","532\n"]}]},{"cell_type":"code","source":["splitfolders.ratio(\"Dataset/training_data_final\", output=\"Dataset/splitted_data_new\",  #split the dataset without balancing the class\n","                   seed=seed, ratio=(.8, .2), group_prefix=None, move=False)"],"metadata":{"id":"ircZOXzY6ZyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splitfolders.fixed(\"Dataset/training_data_final\", output=\"Dataset/splitted_data_balanced_new\", #split the dataset with class balancing\n","    seed=seed, fixed=(50), oversample=True)"],"metadata":{"id":"ETCoZ5eD6lUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splitfolders.fixed(\"Dataset/training_data_final\", output=\"Dataset/splitted_data_balanced_val\", #split the dataset with class balancing\n","    seed=seed, fixed=(50), oversample=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITfQ10stv3Ek","executionInfo":{"status":"ok","timestamp":1668624230128,"user_tz":-60,"elapsed":84867,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"c5f52bc4-89fc-4d1a-b0a9-c715f8eaaa68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Copying files: 3542 files [01:24, 41.97 files/s]\n"]}]},{"cell_type":"code","source":["!find ./Dataset/splitted_data_balanced_val/ -type d -print -exec sh -c \"ls {} | wc -l | head -n1\" \\; #verfify if it's all right"],"metadata":{"id":"Ew9N7pUp6ueJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668624250578,"user_tz":-60,"elapsed":528,"user":{"displayName":"Alessandro Pindozzi","userId":"14537327468545632655"}},"outputId":"4d7431e8-d447-40aa-a521-88c70a61d274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./Dataset/splitted_data_balanced_val/\n","2\n","./Dataset/splitted_data_balanced_val/train\n","8\n","./Dataset/splitted_data_balanced_val/train/Species7\n","487\n","./Dataset/splitted_data_balanced_val/train/Species8\n","458\n","./Dataset/splitted_data_balanced_val/train/Species1\n","136\n","./Dataset/splitted_data_balanced_val/train/Species6\n","172\n","./Dataset/splitted_data_balanced_val/train/Species3\n","465\n","./Dataset/splitted_data_balanced_val/train/Species4\n","461\n","./Dataset/splitted_data_balanced_val/train/Species5\n","481\n","./Dataset/splitted_data_balanced_val/train/Species2\n","482\n","./Dataset/splitted_data_balanced_val/val\n","8\n","./Dataset/splitted_data_balanced_val/val/Species7\n","50\n","./Dataset/splitted_data_balanced_val/val/Species8\n","50\n","./Dataset/splitted_data_balanced_val/val/Species1\n","50\n","./Dataset/splitted_data_balanced_val/val/Species6\n","50\n","./Dataset/splitted_data_balanced_val/val/Species3\n","50\n","./Dataset/splitted_data_balanced_val/val/Species4\n","50\n","./Dataset/splitted_data_balanced_val/val/Species5\n","50\n","./Dataset/splitted_data_balanced_val/val/Species2\n","50\n"]}]},{"cell_type":"markdown","source":["# Keras cv augmentation"],"metadata":{"id":"LuxLQnBb_SYI"}},{"cell_type":"code","source":["!pip install keras_cv\n","import keras_cv\n","!pip install imgaug\n","import imgaug as ia\n","from imgaug import augmenters as iaa"],"metadata":{"id":"1J3oMTwl_hPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_shape = (96, 96, 3)\n","target_size=(96,96)\n","classes = 8\n","epochs = 100\n","BATCH_SIZE = 16\n","AUTO = tf.data.AUTOTUNE\n","IMG_SIZE = 96"],"metadata":{"id":"KUlDFwGMAdHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting dataset directories\n","splitted_data_dir = os.path.join('Dataset/splitted_data_balanced_new')\n","training_dir = os.path.join(splitted_data_dir + '/train')\n","validation_dir = os.path.join(splitted_data_dir + '/val')"],"metadata":{"id":"bdT8T3uXAeiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Labels of the dataset for classification\n","labels = ['Species1',\n","          'Species2',\n","          'Species3',\n","          'Species4',\n","          'Species5',\n","          'Species6',\n","          'Species7',\n","          'Species8']"],"metadata":{"id":"buGKzf4PAf8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Images are divided into folders, one for each class. \n","# If the images are organized in such a way, we can exploit the \n","# ImageDataGenerator to read them from disk.\n","\n","from tensorflow.keras.applications.convnext import preprocess_input\n","\n","# Create an instance of ImageDataGenerator for training, validation, and test sets\n","train_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input)\n","valid_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input)\n","\n","# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","train_gen = train_data_gen.flow_from_directory(directory=training_dir,\n","                                               target_size=target_size,\n","                                               color_mode='rgb',\n","                                               classes=None, # can be set to labels\n","                                               class_mode='categorical',\n","                                               batch_size=BATCH_SIZE,\n","                                               shuffle=True,\n","                                               seed=seed)\n","valid_gen = train_data_gen.flow_from_directory(directory=validation_dir,\n","                                               target_size=target_size,\n","                                               color_mode='rgb',\n","                                               classes=None, # can be set to labels\n","                                               class_mode='categorical',\n","                                               batch_size=BATCH_SIZE,\n","                                               shuffle=False,\n","                                               seed=seed)"],"metadata":{"id":"s89kl2KAAgdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train=np.concatenate([train_gen.next()[0] for i in range(train_gen.__len__())])\n","y_train=np.concatenate([train_gen.next()[1] for i in range(train_gen.__len__())])\n","print(x_train.shape)\n","print(y_train.shape)\n","\n","x_val=np.concatenate([valid_gen.next()[0] for i in range(valid_gen.__len__())])\n","y_val=np.concatenate([valid_gen.next()[1] for i in range(valid_gen.__len__())])\n","print(x_val.shape)\n","print(y_val.shape)"],"metadata":{"id":"oKkBNMud_bp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_image(image, label):\n","    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n","    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0\n","    return image, label"],"metadata":{"id":"Lc_bNqW5Ak-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds_one = (\n","    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    .shuffle(1024)\n","    .map(preprocess_image, num_parallel_calls=AUTO)\n",")\n","train_ds_two = (\n","    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    .shuffle(1024)\n","    .map(preprocess_image, num_parallel_calls=AUTO)\n",")\n","\n","train_ds_simple = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","\n","train_ds_simple = (\n","    train_ds_simple.map(preprocess_image, num_parallel_calls=AUTO)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","# Combine two shuffled datasets from the same training data.\n","train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n","\n","\n","val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","\n","val_ds = (\n","    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    .shuffle(1024)\n","    .map(preprocess_image, num_parallel_calls=AUTO)\n",")"],"metadata":{"id":"M_rzKR4OAmGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_beta_distribution_cutmix(size, concentration_0=0.2, concentration_1=0.2):\n","    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n","    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n","    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n","\n","\n","@tf.function\n","def get_box(lambda_value):\n","    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n","\n","    cut_w = IMG_SIZE * cut_rat  # rw\n","    cut_w = tf.cast(cut_w, tf.int32)\n","\n","    cut_h = IMG_SIZE * cut_rat  # rh\n","    cut_h = tf.cast(cut_h, tf.int32)\n","\n","    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n","    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n","\n","    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n","    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n","    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n","    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n","\n","    target_h = bby2 - boundaryy1\n","    if target_h == 0:\n","        target_h += 1\n","\n","    target_w = bbx2 - boundaryx1\n","    if target_w == 0:\n","        target_w += 1\n","\n","    return boundaryx1, boundaryy1, target_h, target_w\n","\n","\n","@tf.function\n","def cutmix(train_ds_one, train_ds_two):\n","    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n","\n","    alpha = [0.25]\n","    beta = [0.25]\n","\n","    # Get a sample from the Beta distribution\n","    lambda_value = sample_beta_distribution_cutmix(1, alpha, beta)\n","\n","    # Define Lambda\n","    lambda_value = lambda_value[0][0]\n","\n","    # Get the bounding box offsets, heights and widths\n","    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n","\n","    # Get a patch from the second image (`image2`)\n","    crop2 = tf.image.crop_to_bounding_box(\n","        image2, boundaryy1, boundaryx1, target_h, target_w\n","    )\n","    # Pad the `image2` patch (`crop2`) with the same offset\n","    image2 = tf.image.pad_to_bounding_box(\n","        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n","    )\n","    # Get a patch from the first image (`image1`)\n","    crop1 = tf.image.crop_to_bounding_box(\n","        image1, boundaryy1, boundaryx1, target_h, target_w\n","    )\n","    # Pad the `image1` patch (`crop1`) with the same offset\n","    img1 = tf.image.pad_to_bounding_box(\n","        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n","    )\n","\n","    # Modify the first image by subtracting the patch from `image1`\n","    # (before applying the `image2` patch)\n","    image1 = image1 - img1\n","    # Add the modified `image1` and `image2`  together to get the CutMix image\n","    image = image1 + image2\n","\n","    # Adjust Lambda in accordance to the pixel ration\n","    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n","    lambda_value = tf.cast(lambda_value, tf.float32)\n","\n","    # Combine the labels of both images\n","    label = lambda_value * label1 + (1 - lambda_value) * label2\n","    return image, label"],"metadata":{"id":"K0mXITFmAnza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_beta_distribution_mixup(size, concentration_0=0.2, concentration_1=0.2):\n","    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n","    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n","    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n","\n","@tf.function\n","def mix_up(train_ds_one, train_ds_two, alpha=0.2):\n","    # Unpack two datasets\n","    images_one, labels_one = train_ds_one\n","    images_two, labels_two = train_ds_two\n","    batch_size = tf.shape(images_one)[0]\n","\n","    # Sample lambda and reshape it to do the mixup\n","    l = sample_beta_distribution_mixup(batch_size, alpha, alpha)\n","    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n","    y_l = tf.reshape(l, (batch_size, 1))\n","\n","    # Perform mixup on both images and labels by combining a pair of images/labels\n","    # (one from each dataset) into one image/label\n","    images = images_one * x_l + images_two * (1 - x_l)\n","    labels = labels_one * y_l + labels_two * (1 - y_l)\n","    return (images, labels)"],"metadata":{"id":"xU4oBvNsApdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rand_aug = iaa.RandAugment(n=3, m=7)\n","\n","\n","def augment(images):\n","    # Input to `augment()` is a TensorFlow tensor which\n","    # is not supported by `imgaug`. This is why we first\n","    # convert it to its `numpy` variant.\n","    images = tf.cast(images, tf.uint8)\n","    return rand_aug(images=images.numpy())"],"metadata":{"id":"P2H52RbhAqzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Rand augmentation training set\n","train_ds_randaug = (\n","    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    .shuffle(1024)\n","    .batch(BATCH_SIZE)\n","    .map(lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y), num_parallel_calls=AUTO)\n","    .prefetch(AUTO)\n",")"],"metadata":{"id":"KWUu4AGIAsVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_images, _ = next(iter(train_ds_randaug))\n","plt.figure(figsize=(10, 10))\n","for i, image in enumerate(sample_images[:9]):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(image.numpy().astype(\"int\"))\n","    plt.axis(\"off\")"],"metadata":{"id":"8ClN8belAttt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Cut mix training set\n","train_ds_cmu = (\n","    train_ds.shuffle(1024)\n","    .map(cutmix, num_parallel_calls=AUTO)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")"],"metadata":{"id":"f_Flf6uBAvqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualization after cutmix\n","image_batch, label_batch = next(iter(train_ds_cmu))\n","plt.figure(figsize=(10, 10))\n","for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.title(labels[np.argmax(label_batch[i])])\n","    plt.imshow(image_batch[i])\n","    plt.axis(\"off\")"],"metadata":{"id":"ekwGAlE7AxoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Mix up training set\n","train_ds_mu = train_ds.map(\n","    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO\n",")"],"metadata":{"id":"sdwoKP1QAz6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualization after mixup\n","sample_images, sample_labels = next(iter(train_ds_mu))\n","plt.figure(figsize=(10, 10))\n","for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(image.numpy().squeeze())\n","    print(label.numpy().tolist())\n","    plt.axis(\"off\")"],"metadata":{"id":"8FT0ZstKA1b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds_all_aug = tf.data.Dataset.zip((train_ds_randaug, train_ds_cmu, train_ds_mu))\n","combined_dataset1 = train_ds_cmu.concatenate(train_ds_mu)\n","combined_dataset = combined_dataset1.concatenate(train_ds_randaug)"],"metadata":{"id":"cUfDG4-LA3WU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ensembling"],"metadata":{"id":"zGCCmBwsA-KD"}},{"cell_type":"code","source":["class WeightedAverageLayer(tf.keras.layers.Layer):\n","    def __init__(self, w1, w2, w3, **kwargs):\n","        super(WeightedAverageLayer, self).__init__(**kwargs)\n","        self.w1 = w1\n","        self.w2 = w2\n","        self.w3 = w3\n","\n","    def call(self, inputs):\n","        return self.w1 * inputs[0] + self.w2 * inputs[1] + self.w3 * inputs[2]"],"metadata":{"id":"-xy6K7fwBF6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Standarnd_CNN_model = tfk.models.load_model('Standard_NoAug_class_weight_Best')\n","Resnet101_model = tfk.models.load_model('resnet101')\n","ConvnextLarge_model = tfk.models.load_model('convnextlarge') "],"metadata":{"id":"cm1nzjHZBI77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Standarnd_CNN_model._name = 'Standarnd_CNN_model'\n","Standarnd_CNN_model.summary()\n","Resnet101_model._name = 'Resnet101_model'\n","Resnet101_model.summary()\n","ConvnextLarge_model._name = 'ConvnextLarge_model'\n","ConvnextLarge_model.summary()"],"metadata":{"id":"3oqfz1i-BKJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = [Standarnd_CNN_model, Resnet101_model, ConvnextLarge_model]\n","model_input = tf.keras.Input(shape=(96, 96, 3))\n","model_outputs = [model(model_input) for model in models]\n","ensemble_output = WeightedAverageLayer(0.25, 0.25, 0.5)(model_outputs)\n","ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)\n","ensemble_model.save('Ensemble_model')"],"metadata":{"id":"Urn5PE9VBR6y"},"execution_count":null,"outputs":[]}]}